name: MOE LM
release_date: 2021/12/20
affiliation:
  - Meta
language:
  - English
paper/news: https://arxiv.org/abs/2112.10684
domain:
  - Text
parameters_dense:
  - 1.3B
  - 2.7B
  - 6.7B
  - 13B
parameters_MoE:
  - 15B
  - 52B
  - 207B
  - 1100B
model: https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm#pre-trained-models
code: https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm
